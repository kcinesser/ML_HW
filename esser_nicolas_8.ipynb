{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression - Ordinary Least Squares (OLS) Method \n",
    "\n",
    "We will perform linear regression using\n",
    "- Scikit Learn's OLS model\n",
    "- Manually coded OLS method\n",
    "\n",
    "\n",
    "The sklearn OLS implementation code is given in this notebook. You will have to implement the OLS method manually on the given dataset (OLS_Data.csv).\n",
    "\n",
    "\n",
    "### OLS\n",
    "\n",
    "OLS is a type of linear least squares method for estimating the unknown parameters in a linear regression model. OLS chooses the parameters of a linear function of a set of explanatory variables by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being predicted) in the given dataset and those predicted by the linear function.\n",
    "\n",
    "OLS finds the optimal parameters by computing a closed-form solution for the **Normal equation**.\n",
    "\n",
    "URL: https://scikit-learn.org/stable/modules/linear_model.html#linear-model\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We will use a dataset (OLS_Data.csv) containing 14 variables (14 dimensional feature)\n",
    "\n",
    "Input variables:\n",
    "X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14\n",
    "\n",
    "Output variable: \n",
    "y\n",
    "\n",
    "### Note:\n",
    "This dataset might have colinearity in the input variables resulting into the singularity problem. It might cause the OLS method not working. You may need to fix the singularity problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: OLS Linear Regression Using Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from numpy.linalg import det\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "First load the data and explore the feature names, target names, etc.\n",
    "\n",
    "Download the \"OLS_Data.csv\" file to load data from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the csv file as a Pandas DataFrame object denoted as \"df\"\n",
    "\n",
    "df = pd.read_csv('/Users/Nick/datasets/DataScienceRepository/OLS_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Check of the Data\n",
    "\n",
    "Let’s take a look at the top five rows using the DataFrame’s head() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>X12</th>\n",
       "      <th>X13</th>\n",
       "      <th>X14</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        X1    X2    X3  X4     X5     X6    X7      X8  X9  X10   X11     X12  \\\n",
       "0  0.00632  18.0  2.31   0  0.538  6.575  65.2  4.0900   1  296  15.3  396.90   \n",
       "1  0.02731   0.0  7.07   0  0.469  6.421  78.9  4.9671   2  242  17.8  396.90   \n",
       "2  0.02729   0.0  7.07   0  0.469  7.185  61.1  4.9671   2  242  17.8  392.83   \n",
       "3  0.03237   0.0  2.18   0  0.458  6.998  45.8  6.0622   3  222  18.7  394.63   \n",
       "4  0.06905   0.0  2.18   0  0.458  7.147  54.2  6.0622   3  222  18.7  396.90   \n",
       "\n",
       "    X13      X14     y  \n",
       "0  4.98  0.00632  24.0  \n",
       "1  9.14  0.02731  21.6  \n",
       "2  4.03  0.02729  34.7  \n",
       "3  2.94  0.03237  33.4  \n",
       "4  5.33  0.06905  36.2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the Data\n",
    "\n",
    "DataFrame’s info() method is useful to get a quick description of the data, in particular the total number of rows, and each attribute’s type and number of non-null values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 15 columns):\n",
      "X1     506 non-null float64\n",
      "X2     506 non-null float64\n",
      "X3     506 non-null float64\n",
      "X4     506 non-null int64\n",
      "X5     506 non-null float64\n",
      "X6     506 non-null float64\n",
      "X7     506 non-null float64\n",
      "X8     506 non-null float64\n",
      "X9     506 non-null int64\n",
      "X10    506 non-null int64\n",
      "X11    506 non-null float64\n",
      "X12    506 non-null float64\n",
      "X13    506 non-null float64\n",
      "X14    506 non-null float64\n",
      "y      506 non-null float64\n",
      "dtypes: float64(12), int64(3)\n",
      "memory usage: 59.4 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Matrix: Feature Correlations\n",
    "\n",
    "Check if the data matrix has colinearity (1 or close to 1) in its features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X14    1.000000\n",
       "X1     1.000000\n",
       "X9     0.625505\n",
       "X10    0.582764\n",
       "X13    0.455621\n",
       "X5     0.420972\n",
       "X3     0.406583\n",
       "X7     0.352734\n",
       "X11    0.289946\n",
       "X4    -0.055892\n",
       "X2    -0.200469\n",
       "X6    -0.219247\n",
       "X8    -0.379670\n",
       "X12   -0.385064\n",
       "y     -0.388305\n",
       "Name: X1, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()['X1'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Separate Feature Set (Data Matrix X) and Target (1D Vector y)\n",
    "\n",
    "Create a data matrix (X) that contains all features and a 1D target vector (y) containing the target.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14)\n",
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "# data matrix X\n",
    "X = df\n",
    "\n",
    "# target vector y\n",
    "y = (df[\"y\"])\n",
    "\n",
    "X = df.drop(columns=['y'], axis=1)\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale The Features\n",
    "\n",
    "We should ensure that all features have a similar scale. Otherwise optimization algorithms (e.g., Gradient Descent based algorithms) will take much longer time to converge.\n",
    "\n",
    "Also, regularization techniques are sensitive to the scale of data. Thus, we must scale the features before applying regularization.\n",
    "\n",
    "Use sklearns StandardScaler()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nick\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Nick\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scale = scaler.fit(X)\n",
    "X_scale = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train and Test Dataset\n",
    "\n",
    "Create train and test data (80% & 20%) by usinf sklearn's train_test_split function\n",
    "\n",
    "It should return the following 4 matrices.\n",
    "X_train\n",
    "y_train\n",
    "X_test\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Models\n",
    "\n",
    "We will use the following linear regression models.\n",
    "\n",
    "- Ordinary least squares (OLS) Linear Regression (by solving the Normal Equation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "We will use two evaluation metrics.\n",
    "\n",
    "- Mean Squared Error (MSE)\n",
    "- Coefficient of Determination ($R^2$ or $r^2$)\n",
    "\n",
    "\n",
    "### Note on $R^2$:\n",
    "R-squared is a statistical measure of how close the data are to the fitted regression line. \n",
    "\n",
    "R-squared measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n",
    "\n",
    "$R^2 = \\frac{Explained Variation}{Total Variation}$\n",
    "\n",
    "R-squared is always between 0 and 100%:\n",
    "\n",
    "- 0% indicates that the model explains none of the variability of the response data around its mean.\n",
    "- 100% indicates that the model explains all the variability of the response data around its mean.\n",
    "\n",
    "\n",
    "#### <font color=red>In general, the higher the R-squared, the better the model fits your data.</font>\n",
    "\n",
    "\n",
    "#### Compute $R^2$ using the sklearn:\n",
    "\n",
    "- The \"r2_score\" function from sklearn.metrics\n",
    "\n",
    "#### Compute MSE using the sklearn:\n",
    "\n",
    "- The \"mean_squared_error\" function from sklearn.metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Ordinary Least Squares (OLS) Linear Regression (by solving the Normal Equation)\n",
    "\n",
    "\n",
    "#### Sklearn's OLS model implementation code is given for you to review.\n",
    "\n",
    "Then, you will have to manually code the OLS method.\n",
    "\n",
    "\n",
    "#### <font color=red>The MSE and $r^2$ error values from your manually coded OLS method must match with sklearn LinearRegressor's obtained values.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: \n",
      " 22.4852682393169\n",
      "Coefficients: \n",
      " [-0.48574711  0.70155562  0.27675212  0.70653152 -1.99143043  3.11571836\n",
      " -0.17706021 -3.04577065  2.28278471 -1.79260468 -1.97995351  1.12649864\n",
      " -3.62814937 -0.48574711]\n",
      "\n",
      "----------------------------- Model Evaluation -----------------------------\n",
      "\n",
      "Mean squared error: 21.64\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.75\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Create the sklearn OLS linear regression object\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "\n",
    "# Train the model\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# The intercept\n",
    "print(\"Intercept: \\n\", lin_reg.intercept_)\n",
    "\n",
    "# The coefficients\n",
    "print(\"Coefficients: \\n\", lin_reg.coef_)\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Model Evaluation -----------------------------\")\n",
    "\n",
    "\n",
    "# Make prediction \n",
    "y_train_predicted = lin_reg.predict(X_train)\n",
    "\n",
    "\n",
    "print(\"\\nMean squared error: %.2f\"\n",
    "      % mean_squared_error(y_train, y_train_predicted))\n",
    "\n",
    "\n",
    "# To compute \n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % r2_score(y_train, y_train_predicted))\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % lin_reg.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Sklearn OLS Model Using Test Data \n",
    "\n",
    "We evaluate the trained model on the test data.\n",
    "\n",
    "The goal is to see how the model performs on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 24.29\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.67\n"
     ]
    }
   ],
   "source": [
    "# Make prediction \n",
    "y_test_predicted = lin_reg.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_test_predicted))\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % r2_score(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Coded OLS Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Determinant of (X_bias^T.X_bias):  1.0177391434870025e+20\n",
      "\n",
      "The weight vector:\n",
      " [22.4855023   0.80493076  0.70155562  0.27675212  0.70653152 -1.99143043\n",
      "  3.11571836 -0.17706021 -3.04577065  2.28278471 -1.79260468 -1.97995351\n",
      "  1.12649864 -3.62814937 -0.64509273]\n",
      "\n",
      "----------------------------- Model Evaluation -----------------------------\n",
      "Mean squared error: 23.00333612463021\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.74\n"
     ]
    }
   ],
   "source": [
    "# Manually code the OLS Method for Linear Regression\n",
    "\n",
    "\n",
    "# Add a bias term with the feature vectors to create a new data matrix \"X_train_bias\"\n",
    "X_bias = np.c_[np.ones((X_train.shape[0],1)),X_train]\n",
    "\n",
    "# Print the determinant of the dot product of the transpose of X_train_bias and X_train_bias\n",
    "print(\"\\nDeterminant of (X_bias^T.X_bias): \", det(X_bias.T.dot(X_bias)))\n",
    "\n",
    "\n",
    "# Computes the dot product of the transpose of X_train_bias with itself\n",
    "#  Denote the product as \"z\"\n",
    "z = X_bias.T.dot(X_bias)\n",
    "\n",
    "\n",
    "# Closed form solution for weight vector w using Ridge Regregression\n",
    "\n",
    "w = np.linalg.inv(z).dot(X_bias.T).dot(y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nThe weight vector:\\n\", w)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Model Evaluation -----------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "# Make prediction using the X_train_bias data matrix\n",
    "# The predicted target vector should be named as \"y_train_predicted\"\n",
    "y_train_predicted = X_bias.dot(w)\n",
    "\n",
    "\n",
    "# Compute the MSE\n",
    "print(\"Mean squared error:\", mean_squared_error(y_train, y_train_predicted))\n",
    "\n",
    "\n",
    "# Compute the r^2 score\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % r2_score(y_train, y_train_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation on the Performance of the Manually Coded OLS Solution\n",
    "\n",
    "You might get the **Singularity matrix** error.\n",
    "\n",
    "The determinant of the $X_{bias}^T.X_{bias}$ should be 0.\n",
    "\n",
    "There must be colinearity in the columns of the data matrix X.\n",
    "\n",
    "Find which columns are coliner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying OLS Method on Data Matrix With Colinearity in Columns\n",
    "\n",
    "Solve the singularity problem can by adding small positive numbers on the diagonal of the $X_{bias}^T.X_{bias}$ matrix.\n",
    "\n",
    "This regularization technique is known as **Ridge Regression**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Determinant of (X_train_bias^T.X_train_bias):  1.0177391434870025e+20\n",
      "The diagonal matrix:\n",
      " [[100000.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.]\n",
      " [     0. 100000.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.]\n",
      " [     0.      0. 100000.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.]\n",
      " [     0.      0.      0. 100000.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.]\n",
      " [     0.      0.      0.      0. 100000.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.]\n",
      " [     0.      0.      0.      0.      0. 100000.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.]\n",
      " [     0.      0.      0.      0.      0.      0. 100000.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.]\n",
      " [     0.      0.      0.      0.      0.      0.      0. 100000.      0.\n",
      "       0.      0.      0.      0.      0.      0.]\n",
      " [     0.      0.      0.      0.      0.      0.      0.      0. 100000.\n",
      "       0.      0.      0.      0.      0.      0.]\n",
      " [     0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "  100000.      0.      0.      0.      0.      0.]\n",
      " [     0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0. 100000.      0.      0.      0.      0.]\n",
      " [     0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0. 100000.      0.      0.      0.]\n",
      " [     0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0. 100000.      0.      0.]\n",
      " [     0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0. 100000.      0.]\n",
      " [     0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0. 100000.]]\n",
      "\n",
      "-------- Fixing the Singularity of (X_bias^T).X_bias ------------\n",
      "\n",
      "The weight vector:\n",
      " [ 0.09170736 -0.01437852  0.01201041 -0.01931415  0.0082001  -0.01399431\n",
      "  0.03071478 -0.01237209  0.00908906 -0.01593964 -0.0188475  -0.02447065\n",
      "  0.01212677 -0.0296035  -0.01437852]\n",
      "\n",
      "----------------------------- Model Evaluation -----------------------------\n",
      "Mean squared error: 600.0781533963483\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: -5.91\n"
     ]
    }
   ],
   "source": [
    "# Bayesian (Regularized) OLS Method for Linear Regression: Ridge Regression\n",
    "\n",
    "\n",
    "\n",
    "# Add a bias term with the feature vectors to create a new data matrix \"X_train_bias\"\n",
    "X_train_bias = np.c_[np.ones((X_train.shape[0],1)),X_train]\n",
    "\n",
    "\n",
    "# Print the determinant of the dot product of the transpose of X_train_bias and X_train_bias\n",
    "print(\"\\nDeterminant of (X_train_bias^T.X_train_bias): \", det(X_train_bias.T.dot(X_train_bias)))\n",
    "\n",
    "\n",
    "# Computes the dot product of the transpose of X_train_bias with itself\n",
    "#  Denote the product as \"z\"\n",
    "\n",
    "z = X_train_bias.T.dot(X_train_bias) \n",
    "\n",
    "\n",
    "# Closed form (OLS) solution for weight vector w \n",
    "\n",
    "# Create a diagonal matrix that has the dimension of z\n",
    "diagonal = np.eye(z.shape[0], dtype=float)\n",
    "\n",
    "# Add small non-zero numbers on the diagonal\n",
    "diagonal = diagonal * 100000\n",
    "print(\"The diagonal matrix:\\n\", diagonal)\n",
    "\n",
    "\n",
    "print(\"\\n-------- Fixing the Singularity of (X_bias^T).X_bias ------------\")\n",
    "\n",
    "# Create a diagonal matrix that has the dimension of z; name the matrix as \"diagonal\"\n",
    "\n",
    "\n",
    "# Add small positive non-zero numbers on the diagonal\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Closed form (OLS) solution for weight vector w \n",
    "w = np.linalg.inv(z + diagonal).dot(X_train_bias.T).dot(y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nThe weight vector:\\n\", w)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Model Evaluation -----------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "# Make prediction using the X_train_bias data matrix\n",
    "# The predicted target vector should be named as \"y_train_predicted\"\n",
    "y_train_predicted = X_train_bias.dot(w)\n",
    "\n",
    "\n",
    "# Compute the MSE\n",
    "print(\"Mean squared error:\", mean_squared_error(y_train, y_train_predicted))\n",
    "\n",
    "\n",
    "# Compute the r^2 score\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % r2_score(y_train, y_train_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model Using Test Data - OLS Linear Regression\n",
    "\n",
    "We evaluate the trained model on the test data.\n",
    "\n",
    "Compute the MSE and $r^2$ score using the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Determinant of (X_test_bias^T.X_test_bias):  -1.8592240305725827e-06\n",
      "The diagonal matrix:\n",
      " [[0.001 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.   ]\n",
      " [0.    0.001 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.   ]\n",
      " [0.    0.    0.001 0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.   ]\n",
      " [0.    0.    0.    0.001 0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.001 0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.001 0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.001 0.    0.    0.    0.    0.\n",
      "  0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.001 0.    0.    0.    0.\n",
      "  0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.001 0.    0.    0.\n",
      "  0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.001 0.    0.\n",
      "  0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.001 0.\n",
      "  0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.001\n",
      "  0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.001 0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.001 0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.001]]\n",
      "\n",
      "-------- Fixing the Singularity of (X_bias^T).X_bias ------------\n",
      "\n",
      "The weight vector:\n",
      " [22.46366388 -0.30927553  2.70411065  0.38260091  0.3533188  -3.17065568\n",
      "  0.45338     0.90575277 -3.30260939  3.30007128 -2.35714421 -2.56313055\n",
      " -0.28161462 -4.64879606 -0.30927553]\n",
      "\n",
      "----------------------------- Model Evaluation -----------------------------\n",
      "Mean squared error: 17.19296840341662\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.77\n"
     ]
    }
   ],
   "source": [
    "# Bayesian (Regularized) OLS Method for Linear Regression: Ridge Regression\n",
    "\n",
    "\n",
    "\n",
    "# Add a bias term with the feature vectors to create a new data matrix \"X_train_bias\"\n",
    "X_test_bias = np.c_[np.ones((X_test.shape[0],1)),X_test]\n",
    "\n",
    "\n",
    "# Print the determinant of the dot product of the transpose of X_train_bias and X_train_bias\n",
    "print(\"\\nDeterminant of (X_test_bias^T.X_test_bias): \", det(X_test_bias.T.dot(X_test_bias)))\n",
    "\n",
    "\n",
    "# Computes the dot product of the transpose of X_train_bias with itself\n",
    "#  Denote the product as \"z\"\n",
    "\n",
    "z = X_test_bias.T.dot(X_test_bias) \n",
    "\n",
    "\n",
    "# Closed form (OLS) solution for weight vector w \n",
    "\n",
    "# Create a diagonal matrix that has the dimension of z\n",
    "diagonal = np.eye(z.shape[0], dtype=float)\n",
    "\n",
    "# Add small non-zero numbers on the diagonal\n",
    "diagonal = diagonal * 0.001\n",
    "print(\"The diagonal matrix:\\n\", diagonal)\n",
    "\n",
    "\n",
    "print(\"\\n-------- Fixing the Singularity of (X_bias^T).X_bias ------------\")\n",
    "\n",
    "# Create a diagonal matrix that has the dimension of z; name the matrix as \"diagonal\"\n",
    "\n",
    "\n",
    "# Add small positive non-zero numbers on the diagonal\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Closed form (OLS) solution for weight vector w \n",
    "w = np.linalg.inv(z + diagonal).dot(X_test_bias.T).dot(y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nThe weight vector:\\n\", w)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Model Evaluation -----------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "# Make prediction using the X_train_bias data matrix\n",
    "# The predicted target vector should be named as \"y_train_predicted\"\n",
    "y_test_predicted = X_test_bias.dot(w)\n",
    "\n",
    "\n",
    "# Compute the MSE\n",
    "print(\"Mean squared error:\", mean_squared_error(y_test, y_test_predicted))\n",
    "\n",
    "\n",
    "# Compute the r^2 score\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % r2_score(y_test, y_test_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Understanding the Singularity Issue and its Solution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Why do you think the singularity matrix error occur while using OLS method on the “OLS_Data.csv” dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Since the determinant is 0, it is not invertable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) To fix the singularity problem of the $X_{bias}^T.X_{bias}$ matrix what non-zero positive number did you add on its diagonal?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: 0.001 on the diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Add 100000 on the diagonal of the $X_{bias}^T.X_{bias}$ matrix and report the $MSE$ and the $r^2$ values for the training data set. Explain these results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: I got 68.49.<br> Increasing the diagonal increases the penalty resulting in a determinant further away from 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) After adding 100000 on the diagonal of the $X_{bias}^T.X_{bias}$ matrix what change did you notice in the weights of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: The weights got smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
